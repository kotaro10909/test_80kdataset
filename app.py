import streamlit as st
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
from streamlit.logger import get_logger
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import Any, List
from langchain_core.retrievers import BaseRetriever  # LangChainæœ€æ–°ç‰ˆç”¨
from pydantic import BaseModel
from langchain.schema import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd
from typing import List, Any
from pgvector.psycopg2 import register_vector
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain_community.utilities import SQLDatabase
from langchain_google_genai import ChatGoogleGenerativeAI
from sqlalchemy import create_engine
logger = get_logger(__name__)
class Embedding:
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = HuggingFaceEmbeddings(model_name=self.model_name)

    def embed_query(self, text: str) -> list[float]:
        return self.model.embed_query(text)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self.model.embed_documents(texts)


class PostgresVectorRetriever(BaseRetriever, BaseModel):
    conn: Any
    table_name: str
    embedding_function: Any  # GoogleEmbedding ãªã©
    k: int = 10

    model_config = ConfigDict(arbitrary_types_allowed=True)  # â† Pydanticã®è¨­å®š

    def get_embedding(self, input_texts: list[str]) -> list[list[float]]:
        return self.embedding_function.embed_documents(input_texts)
    
    def update_vector(self) -> List[Document]:
        # 1. æœªç™»éŒ²ãƒ‡ãƒ¼ã‚¿ã«embeddingã‚’ä»˜ä¸ã—ã¦æ›´æ–°
        df = pd.read_sql(
            f'SELECT item_id, item_name, item_detail FROM "{self.table_name}" WHERE embedding IS NULL LIMIT 10',
            self.conn
        )

        if not df.empty:
            for i, row in df.iterrows():
                try:
                    # 1ä»¶ãšã¤åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingã‚¯ãƒ©ã‚¹ã‚’ä½¿ã†ï¼‰
                    text = f"{row['item_name']} {row['item_detail']}"
                    vec = self.embedding_function.embed_query(text)

                    print(f"{i}: {row['item_name']} -> {vec[:5]}")  # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆå…ˆé ­5æ¬¡å…ƒã®ã¿ï¼‰

                    update_query = text(f"""
                        UPDATE "{self.table_name}" SET embedding = :embedding WHERE item_id = :item_id
                    """)
                    self.conn.execute(update_query, {
                        "embedding": vec,
                        "item_id": row["item_id"]
                    })
                    self.conn.commit()

                except Exception as e:
                    print(f"Embedding error at index {i}: {e}")
                    continue




    def get_relevant_documents(self, query: str) -> List[Document]:
        
        #  æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_vector = self.embedding_function.embed_query(query)

        ## ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’PostgreSQLå½¢å¼ã«å¤‰æ›
        # query_vector: List[float]
        vector_str = f"[{', '.join(map(str, query_vector))}]"

        search_sql = f"""
            SELECT item_name, item_detail,
                1 - (embedding <-> '{vector_str}'::vector) AS similarity
            FROM "{self.table_name}"
            ORDER BY embedding <-> '{vector_str}'::vector
            LIMIT {self.k}
        """
        result_df = pd.read_sql(search_sql, self.conn)

        # çµæœã‚’ Document ã«ã—ã¦è¿”ã™
        return [
            Document(
                page_content=row["item_detail"],
                metadata={
                    "title": row["item_name"],
                    "similarity": row["similarity"]
                }
            )
            for _, row in result_df.iterrows()
        ]




embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt_template = """
è³ªå•ã®å†…å®¹ã¨ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èƒŒæ™¯æƒ…å ±ã®ä¸­ã‹ã‚‰å›ç­”ã‚’é–¢é€£åº¦ã®é«˜ã„é †ç•ªã§3ã¤å€™è£œå‡ºã—ã¦ãã ã•ã„ï¼ˆå…ˆé ­ã®æ–‡å­—ã¯ã€1. 2. 3.ã¨ç¶šãã‚ˆã†ã«ï¼‰ã€‚
åŒã˜è¡Œã®å†…å®¹ã‚‚è¡¨ç¤ºã™ã‚‹ï¼ˆä¾‹ã€€ã€€1.å•†å“1ï¼šã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆï¼‰ã€‚
# èƒŒæ™¯æƒ…å ±
{context}

# è³ªå•
{question}
"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-lite",
    temperature=0.0,
    max_retries=2,
)


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Vector Database Demo",
    page_icon="ğŸ”",
    layout="wide"
)
def run_qa_from_db(search_text):
    raw_conn = psycopg2.connect("x")
    register_vector(raw_conn)
    raw_conn.close()
    db_uri = "postgresql+psycopg2://xx"
    engine = create_engine(db_uri)
    conn = engine.connect()
    table_name = "80kdatatest"
    query = f'SELECT item_id,item_name,item_detail FROM "{table_name}" LIMIT 1000'
    df = pd.read_sql(query, conn)
    if df.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    # CSVã¨ã—ã¦ä¿å­˜ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¯é©å®œå¤‰æ›´ï¼‰
    # csv_path = "output_documents.csv"
    # df.to_csv(csv_path, index=False, encoding="utf-8")
    # DataFrameã®å†…å®¹ã‚’CSVé¢¨ã®æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤å¤–ï¼‰
    csv_text = "\n".join([
        f"{row['item_id']} / {row['item_name']} / {row['item_detail']}"
        for _, row in df.iterrows()
    ])
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=50000,
        chunk_overlap=1,
        length_function=len,
        separators=["\n"],  # æŸ”è»Ÿã«å¯¾å¿œ
    )

    all_chunks = []

    # csv_files = glob.glob('output_documents.csv')
    # for csv_path in csv_files:
    #     with open(csv_path, newline="", encoding="utf-8-sig") as csvfile:
    #         reader = csv.reader(csvfile)
    #         headers = next(reader)
    #         csv_text = "\n".join([" / ".join(row) for row in reader])

    csv_chunks = text_splitter.split_text(csv_text)
    for i, chunk in enumerate(csv_chunks):
        doc = Document(page_content=chunk, metadata={"source_csv": "postgresDB", "chunk_index": i})
        all_chunks.append(doc)

    print(f"âœ… CSVãƒãƒ£ãƒ³ã‚¯æ•°: {len(csv_chunks)}")
    # FAISSãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
    # vectorstore = FAISS.from_documents(all_chunks, embedding)
    # retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    retriever = PostgresVectorRetriever(
    conn=conn,
    embedding_function=embedding,  # LangChainã®Embeddingï¼ˆä¾‹: GoogleEmbeddingãªã©ï¼‰
    table_name=table_name,
    k=10
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT},
    )
    retriever.update_vector()
    docs = retriever.get_relevant_documents("ãƒ™ã‚¯ãƒˆãƒ«è©³ç´°ãŒçŸ¥ã‚ŠãŸã„")
    for doc in docs:
        print(doc.metadata, doc.page_content[:50])
    # response = qa.invoke(search_text)
    # å›ç­”ã®è¡¨ç¤º
    with st.chat_message("assistant"):
        st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
    with st.chat_message("assistant"):
        for idx, doc in enumerate(docs):
            with st.expander(f"å€™è£œ {idx+1}ï¼ˆé¡ä¼¼åº¦: {doc.metadata['similarity']:.3f}ï¼‰"):
                st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {doc.metadata['title']}")
                st.markdown(doc.page_content[:100] + "...")

def get_texttosql(question):
    # PostgreSQLã¸ã®æ¥ç¶š
    try:
        pg = st.secrets["connections"]["postgresql"]
        db_uri = f"{pg['dialect']}+psycopg2://{pg['username']}:{pg['password']}@{pg['host']}:{pg['port']}/{pg['database']}"
        db = SQLDatabase.from_uri(db_uri, include_tables=["80kdatatest"])

        # LLMã®è¨­å®šï¼ˆGeminiï¼‰
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0,
        )

        # SQL Agentã®ä½œæˆ
        toolkit = SQLDatabaseToolkit(db=db, llm=llm)
        agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True, handle_parsing_errors=True)

        # è³ªå•ã®å®Ÿè¡Œ
        request_sql = f"""
            {question}ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹(èª¤å­—è„±å­—ã‚’è€ƒæ…®ã™ã‚‹)å•†å“ã‚’æœ€å¤§3ä»¶ã¾ã§é¸ã³ã€item_id, item_name, item_detailã‚’å«ã‚€çµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            SQLã¯å¿…ãš SELECT æ–‡ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã¨ãã¯ã€ã€Œè©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
            å‡ºåŠ›ã¯ä»¥ä¸‹ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ï¼š

            1. <item_id> / <item_name> / <item_detail>  
            2. <item_id> / <item_name> / <item_detail>  
            3. <item_id> / <item_name> / <item_detail>
            """
        response = agent_executor.run(request_sql)
        return response
    except Exception as e:
                    print(f"error occured {e}")
                    return None


def get_embedding(text):
    return embedding.embed_query(text)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–¢æ•°
def get_connection():
    return st.connection('postgresql', type='sql')

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
def main():
    st.title("ğŸ“Š Postgres AI Search")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®æ“ä½œé¸æŠ
    operation = st.sidebar.selectbox(
        "æ“ä½œã‚’é¸æŠ",
        ["ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿è¿½åŠ ", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"]
    )
    
    if operation == "ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º":
        show_data()
    elif operation == "ãƒ‡ãƒ¼ã‚¿è¿½åŠ ":
        add_data()
    elif operation == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢":
        vector_search()

def show_data():
    st.header("ğŸ“‹ ç™»éŒ²ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")
    
    conn = get_connection()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    table_name = "80kdatatest"
    query = f'SELECT item_id,item_name,item_detail FROM "{table_name}" LIMIT 100'
    df = conn.query(query, ttl=0)
    
    if not df.empty:
        st.dataframe(df)
    else:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")

def add_data():
    st.header("â• ãƒ‡ãƒ¼ã‚¿è¿½åŠ ")
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("data_form"):
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›")
        content = st.text_area("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±ã‚’å…¥åŠ›")
        submitted = st.form_submit_button("ç™»éŒ²")
        
        if submitted and content:
            conn = get_connection()
            
            # ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãª1536æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€é©åˆ‡ãªã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
            embedding = get_embedding(title + " " + content)
            
            # ãƒ‡ãƒ¼ã‚¿ç™»éŒ²
            query = text("""
            INSERT INTO documents (title, content, embedding)
            VALUES (:title, :content, :embedding)
            """)
            params = {"title": title, "content": content, "embedding": embedding}
            
            try:
                with conn.session as session:
                    session.execute(query, params)
                    session.commit()
                st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def vector_search():
    st.header("ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢")
    search_text = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if st.button("æ¤œç´¢") and search_text:
        result = get_texttosql(search_text)
        # with st.chat_message("assistant"):
        #     st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
        with st.chat_message("assistant"):
            if result:
                st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
                st.markdown(result)
            else:
                st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

       

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    main()


