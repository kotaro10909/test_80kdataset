import streamlit as st
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
from streamlit.logger import get_logger
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import Any, List
from langchain_core.retrievers import BaseRetriever  # LangChainæœ€æ–°ç‰ˆç”¨
from pydantic import BaseModel
from langchain.schema import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd
from typing import List, Any
from pgvector.psycopg2 import register_vector
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain_community.utilities import SQLDatabase
from langchain_google_genai import ChatGoogleGenerativeAI
from sqlalchemy import create_engine
logger = get_logger(__name__)
class Embedding:
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = HuggingFaceEmbeddings(model_name=self.model_name)

    def embed_query(self, text: str) -> list[float]:
        return self.model.embed_query(text)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self.model.embed_documents(texts)


class PostgresVectorRetriever(BaseRetriever, BaseModel):
    conn: Any
    table_name: str
    embedding_function: Any  # GoogleEmbedding ãªã©
    k: int = 10

    model_config = ConfigDict(arbitrary_types_allowed=True)  # â† Pydanticã®è¨­å®š

    def get_embedding(self, input_texts: list[str]) -> list[list[float]]:
        return self.embedding_function.embed_documents(input_texts)
    
    def update_vector(self) -> List[Document]:
        # 1. æœªç™»éŒ²ãƒ‡ãƒ¼ã‚¿ã«embeddingã‚’ä»˜ä¸ã—ã¦æ›´æ–°
        df = pd.read_sql(
            f'SELECT item_id, item_name, item_detail FROM "{self.table_name}" WHERE embedding IS NULL LIMIT 10',
            self.conn
        )

        if not df.empty:
            for i, row in df.iterrows():
                try:
                    # 1ä»¶ãšã¤åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingã‚¯ãƒ©ã‚¹ã‚’ä½¿ã†ï¼‰
                    text = f"{row['item_name']} {row['item_detail']}"
                    vec = self.embedding_function.embed_query(text)

                    print(f"{i}: {row['item_name']} -> {vec[:5]}")  # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆå…ˆé ­5æ¬¡å…ƒã®ã¿ï¼‰

                    update_query = text(f"""
                        UPDATE "{self.table_name}" SET embedding = :embedding WHERE item_id = :item_id
                    """)
                    self.conn.execute(update_query, {
                        "embedding": vec,
                        "item_id": row["item_id"]
                    })
                    self.conn.commit()

                except Exception as e:
                    print(f"Embedding error at index {i}: {e}")
                    continue




    def get_relevant_documents(self, query: str) -> List[Document]:
        
        #  æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_vector = self.embedding_function.embed_query(query)

        ## ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚’PostgreSQLå½¢å¼ã«å¤‰æ›
        # query_vector: List[float]
        vector_str = f"[{', '.join(map(str, query_vector))}]"

        search_sql = f"""
            SELECT item_name, item_detail,
                1 - (embedding <-> '{vector_str}'::vector) AS similarity
            FROM "{self.table_name}"
            ORDER BY embedding <-> '{vector_str}'::vector
            LIMIT {self.k}
        """
        result_df = pd.read_sql(search_sql, self.conn)

        # çµæœã‚’ Document ã«ã—ã¦è¿”ã™
        return [
            Document(
                page_content=row["item_detail"],
                metadata={
                    "title": row["item_name"],
                    "similarity": row["similarity"]
                }
            )
            for _, row in result_df.iterrows()
        ]




embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt_template = """
è³ªå•ã®å†…å®¹ã¨ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èƒŒæ™¯æƒ…å ±ã®ä¸­ã‹ã‚‰å›ç­”ã‚’é–¢é€£åº¦ã®é«˜ã„é †ç•ªã§3ã¤å€™è£œå‡ºã—ã¦ãã ã•ã„ï¼ˆå…ˆé ­ã®æ–‡å­—ã¯ã€1. 2. 3.ã¨ç¶šãã‚ˆã†ã«ï¼‰ã€‚
åŒã˜è¡Œã®å†…å®¹ã‚‚è¡¨ç¤ºã™ã‚‹ï¼ˆä¾‹ã€€ã€€1.å•†å“1ï¼šã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆï¼‰ã€‚
# èƒŒæ™¯æƒ…å ±
{context}

# è³ªå•
{question}
"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-lite",
    temperature=0.0,
    max_retries=2,
)


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Vector Database Demo",
    page_icon="ğŸ”",
    layout="wide"
)
def run_qa_from_db(search_text):
    raw_conn = psycopg2.connect("dbname=first_test user=user_kotaro password=fractal_first_test")
    register_vector(raw_conn)
    raw_conn.close()
    db_uri = "postgresql+psycopg2://user_kotaro:fractal_first_test@localhost:5432/first_test"
    engine = create_engine(db_uri)
    conn = engine.connect()
    table_name = "80kdatatest"
    query = f'SELECT item_id,item_name,item_detail FROM "{table_name}" LIMIT 1000'
    df = pd.read_sql(query, conn)
    if df.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    # CSVã¨ã—ã¦ä¿å­˜ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¯é©å®œå¤‰æ›´ï¼‰
    # csv_path = "output_documents.csv"
    # df.to_csv(csv_path, index=False, encoding="utf-8")
    # DataFrameã®å†…å®¹ã‚’CSVé¢¨ã®æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤å¤–ï¼‰
    csv_text = "\n".join([
        f"{row['item_id']} / {row['item_name']} / {row['item_detail']}"
        for _, row in df.iterrows()
    ])
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=50000,
        chunk_overlap=1,
        length_function=len,
        separators=["\n"],  # æŸ”è»Ÿã«å¯¾å¿œ
    )

    all_chunks = []

    # csv_files = glob.glob('output_documents.csv')
    # for csv_path in csv_files:
    #     with open(csv_path, newline="", encoding="utf-8-sig") as csvfile:
    #         reader = csv.reader(csvfile)
    #         headers = next(reader)
    #         csv_text = "\n".join([" / ".join(row) for row in reader])

    csv_chunks = text_splitter.split_text(csv_text)
    for i, chunk in enumerate(csv_chunks):
        doc = Document(page_content=chunk, metadata={"source_csv": "postgresDB", "chunk_index": i})
        all_chunks.append(doc)

    print(f"âœ… CSVãƒãƒ£ãƒ³ã‚¯æ•°: {len(csv_chunks)}")
    # FAISSãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
    # vectorstore = FAISS.from_documents(all_chunks, embedding)
    # retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    retriever = PostgresVectorRetriever(
    conn=conn,
    embedding_function=embedding,  # LangChainã®Embeddingï¼ˆä¾‹: GoogleEmbeddingãªã©ï¼‰
    table_name=table_name,
    k=10
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT},
    )
    retriever.update_vector()
    docs = retriever.get_relevant_documents("ãƒ™ã‚¯ãƒˆãƒ«è©³ç´°ãŒçŸ¥ã‚ŠãŸã„")
    for doc in docs:
        print(doc.metadata, doc.page_content[:50])
    # response = qa.invoke(search_text)
    # å›ç­”ã®è¡¨ç¤º
    with st.chat_message("assistant"):
        st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
    with st.chat_message("assistant"):
        for idx, doc in enumerate(docs):
            with st.expander(f"å€™è£œ {idx+1}ï¼ˆé¡ä¼¼åº¦: {doc.metadata['similarity']:.3f}ï¼‰"):
                st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {doc.metadata['title']}")
                st.markdown(doc.page_content[:100] + "...")

def get_texttosql(question):
    # PostgreSQLã¸ã®æ¥ç¶š
    try:
        pg = st.secrets["connections"]["postgresql"]
        db_uri = f"{pg['dialect']}+psycopg2://{pg['username']}:{pg['password']}@{pg['host']}:{pg['port']}/{pg['database']}"
        db = SQLDatabase.from_uri(db_uri, include_tables=["80kdatatest"])

        # LLMã®è¨­å®šï¼ˆGeminiï¼‰
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0,
        )

        # SQL Agentã®ä½œæˆ
        toolkit = SQLDatabaseToolkit(db=db, llm=llm)
        agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True, handle_parsing_errors=True)

        # è³ªå•ã®å®Ÿè¡Œ
        request_sql = f"""
            {question}ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹(èª¤å­—è„±å­—ã‚’è€ƒæ…®ã™ã‚‹)å•†å“ã‚’æœ€å¤§3ä»¶ã¾ã§é¸ã³ã€item_id, item_name, item_detailã‚’å«ã‚€çµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            SQLã¯å¿…ãš SELECT æ–‡ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã¨ãã¯ã€ã€Œè©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
            å‡ºåŠ›ã¯ä»¥ä¸‹ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ï¼š

            1. <item_id> / <item_name> / <item_detail>  
            2. <item_id> / <item_name> / <item_detail>  
            3. <item_id> / <item_name> / <item_detail>
            """
        response = agent_executor.run(request_sql)
        return response
    except Exception as e:
                    print(f"error occured {e}")
                    return None


def get_embedding(text):
    return embedding.embed_query(text)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–¢æ•°
def get_connection():
    return st.connection('postgresql', type='sql')

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
def main():
    st.title("ğŸ“Š Postgres AI Search")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®æ“ä½œé¸æŠ
    operation = st.sidebar.selectbox(
        "æ“ä½œã‚’é¸æŠ",
        ["ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿è¿½åŠ ", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"]
    )
    
    if operation == "ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º":
        show_data()
    elif operation == "ãƒ‡ãƒ¼ã‚¿è¿½åŠ ":
        add_data()
    elif operation == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢":
        vector_search()

def show_data():
    st.header("ğŸ“‹ ç™»éŒ²ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")
    
    conn = get_connection()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    table_name = "80kdatatest"
    query = f'SELECT item_id,item_name,item_detail FROM "{table_name}" LIMIT 100'
    df = conn.query(query, ttl=0)
    
    if not df.empty:
        st.dataframe(df)
    else:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")

def add_data():
    st.header("â• ãƒ‡ãƒ¼ã‚¿è¿½åŠ ")
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("data_form"):
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›")
        content = st.text_area("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±ã‚’å…¥åŠ›")
        submitted = st.form_submit_button("ç™»éŒ²")
        
        if submitted and content:
            conn = get_connection()
            
            # ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãª1536æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€é©åˆ‡ãªã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
            embedding = get_embedding(title + " " + content)
            
            # ãƒ‡ãƒ¼ã‚¿ç™»éŒ²
            query = text("""
            INSERT INTO documents (title, content, embedding)
            VALUES (:title, :content, :embedding)
            """)
            params = {"title": title, "content": content, "embedding": embedding}
            
            try:
                with conn.session as session:
                    session.execute(query, params)
                    session.commit()
                st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def vector_search():
    st.header("ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢")
    search_text = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if st.button("æ¤œç´¢") and search_text:
        result = get_texttosql(search_text)
        # with st.chat_message("assistant"):
        #     st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
        with st.chat_message("assistant"):
            if result:
                st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§ã™ã‹ï¼Ÿ")
                st.markdown(result)
            else:
                st.markdown("ãŠæ¢ã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

       

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    main()


# from langchain.chains import RetrievalQA
# from langchain.schema import (SystemMessage, HumanMessage, AIMessage)
# from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
# from langchain_community.vectorstores import FAISS
# import os
# import streamlit as st
# from langchain.prompts import ChatPromptTemplate
# from langchain.prompts.chat import (
#     ChatPromptTemplate,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate,
# )
# from langchain.prompts import PromptTemplate
# import psycopg2
# import numpy as np
# from langchain.vectorstores.pgvector import PGVector

# os.environ["GOOGLE_API_KEY"] = "AIzaSyBnmd9ILsLXBucQM9K9VxUYcVS9ZfI_v60"

# def app_login():
#     # æœªãƒ­ã‚°ã‚¤ãƒ³æ™‚ã®è¡¨ç¤º
#     if not st.user.is_logged_in:
#         if st.button("Googleã§ãƒ­ã‚°ã‚¤ãƒ³"):
#              st.cache_data.clear()
#              st.login()
#              st.rerun()

# def user_check():
#     allowed_emails = st.secrets["access"]["allowed_emails"]
#     if st.user.email not in allowed_emails:
#         st.error("ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
#         if st.button("åˆ¥ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³"):
#              st.logout()
#              st.cache_data.clear()
#              st.rerun()
#         return True
#     return False

# def app_logout():
#         if st.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
#             st.logout()
#             st.cache_data.clear()
#             st.rerun()

# # def load_db(embeddings):
# #     return FAISS.load_local('faiss_store', embeddings, allow_dangerous_deserialization=True)

# def load_db(embeddings):
#     CONNECTION_STRING = "postgresql+psycopg2://user_kotaro:fractal_first_test@localhost:5432/first_test"
#     COLLECTION_NAME = "Shohin"

#     return PGVector(
#         collection_name=COLLECTION_NAME,
#         connection_string=CONNECTION_STRING,
#         embedding_function=embeddings,
#     )


# def init_page():
#     st.set_page_config(
#         page_title='FRACTAL AI SEARCH',
#         page_icon='ğŸ§‘â€ğŸ’»',
#     )


# def main():
#     init_page()
#   # if not st.user.is_logged_in:
#   #   app_login()
#   # else:
#   #   if user_check():
#   #       return
#   #   app_logout()
#     # DBæ¥ç¶šæƒ…å ±
#     conn = psycopg2.connect(
#         host="localhost",
#         dbname="first_test",
#         user="user_kotaro",
#         password="fractal_first_test",
#         port=5432,
#     )

#     cur = conn.cursor()

#     # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ¢ãƒ‡ãƒ«
#     embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

#     # å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—
#     cur.execute("SELECT id, title, description FROM Shohin")
#     rows = cur.fetchall()
#     print(rows)
#     for row in rows:
#         id_, title, description = row
#         text = title + " " + description
#         vector = embeddings.embed_query(text)  # list[float]
#         # ç›´æ¥ vector ã‚’æ¸¡ã™ï¼ˆæ–‡å­—åˆ—ã«ã—ãªã„ï¼‰
#         cur.execute(
#             "UPDATE Shohin SET embedding = %s WHERE id = %s",
#             (vector, id_)
#         )


#     conn.commit()
#     cur.close()
#     conn.close()
#     db = load_db(embeddings)

#     llm = ChatGoogleGenerativeAI(
#         model="gemini-2.0-flash-lite",
#         temperature=0.0,
#         max_retries=2,
#     )

#     # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®System Instructionã‚’å®šç¾©ã™ã‚‹

#     # prompt_template = """
#     # ã‚ãªãŸã¯ã€ã€ŒåŒ—è¾°ç‰©ç”£ã€ã¨ã„ã†å›£ä½“å°‚ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
#     # èƒŒæ™¯æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›£ä½“ã®äººé–“ã«ãªã‚Šãã£ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã„ã€‚è§£ç­”ã®æœ€å¾Œã§ã€å‚ç…§ã—ãŸURLãŒåˆ†ã‹ã‚Œã°ãã‚Œã‚’ç­”ãˆã¦ãã ã•ã„ã€‚
#     # ä»¥ä¸‹ã®èƒŒæ™¯æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚èƒŒæ™¯æƒ…å ±ã¨é–¢ä¿‚ã®ã‚ã‚‹è³ªå•ã«ã¯ç­”ãˆã¦ãã ã•ã„ã€‚
#     # æƒ…å ±ãŒãªã‘ã‚Œã°ã€ãã®å†…å®¹ã«ã¤ã„ã¦ã¯è¨€åŠã—ãªã„ã§ãã ã•ã„ã€‚
#     # # èƒŒæ™¯æƒ…å ±
#     # {context}

#     # # è³ªå•
#     # {question}"""
#     prompt_template = """
#     ã‚ãªãŸã¯ã€ã€Œãƒ•ãƒ©ã‚¯ã‚¿ãƒ«å»ºç¯‰ã€ã¨ã„ã†å›£ä½“å°‚ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
#     èƒŒæ™¯æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›£ä½“ã®äººé–“ã«ãªã‚Šãã£ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã„ã€‚å›ç­”æƒ…å ±ã¨å¯¾å¿œã™ã‚‹DBã®ã‚«ãƒ©ãƒ ã®æƒ…å ±ã‚‚åŠ ãˆã¦ãã ã•ã„ã€‚
#     ä»¥ä¸‹ã®èƒŒæ™¯æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚èƒŒæ™¯æƒ…å ±ã¨é–¢ä¿‚ã®ã‚ã‚‹è³ªå•ã«ã¯ç­”ãˆã¦ãã ã•ã„ã€‚
#     æƒ…å ±ãŒãªã‘ã‚Œã°ã€ãã®å†…å®¹ã«ã¤ã„ã¦ã¯è¨€åŠã—ãªã„ã§ãã ã•ã„ã€‚
#     # èƒŒæ™¯æƒ…å ±
#     {context}

#     # è³ªå•
#     {question}"""
#     PROMPT = PromptTemplate(
#         template=prompt_template, input_variables=["context", "question"]
#     )
#     qa = RetrievalQA.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=db.as_retriever(search_kwargs={"k": 3}),
#         return_source_documents=True,
#         chain_type_kwargs={"prompt": PROMPT}# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
#     )

#     # ä¸ŠåŠåˆ†ã«èª¬æ˜æ–‡ã¨ãƒªãƒ³ã‚¯ã‚’é…ç½®
#     # HTMLã‚¿ã‚°ã‚’ãã®ã¾ã¾ä½¿ã£ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
#     st.markdown("""
#         <style>
#         .chat-box1 {
#             overflow-y: auto;
#             border: 2px solid #ddd;
#             padding: 20px;
#             border-radius: 8px;
#             background-color: none;
#             margin-bottom: 20px;
#             text-align: center;
#             background-color = #f5f5f5;
#             font: monospace;
#             color: #333333;
#         }
#         .chat-box1 h1{
#             color: #00536D;
#         }
#         </style>
#         """, unsafe_allow_html=True)

#     # ç›´æ¥HTMLè¦ç´ ã‚’è¡¨ç¤º
#     st.markdown("""
#         <div class="chat-box1">
#             <h1>FRACTAL AI SEARCH</h1>
#             <p>ã“ã®AIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã¯ã€ç¤¾å†…æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚</p>
#             <p>ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«å»ºç¯‰ã«é–¢ã™ã‚‹è³ªå•ä»¥å¤–ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚ï¼ˆå®Ÿéš›ã®ä¼æ¥­åã‚„ç¤¾å†…æƒ…å ±ã¯å…¥ã£ã¦ã„ã¾ã›ã‚“ã€‚ï¼‰</p>
#             <a href="#">ç¤¾å†…ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆ</a>
#         </div>
#     """, unsafe_allow_html=True)

#     # ä¸‹åŠåˆ†ã«ãƒãƒ£ãƒƒãƒˆç”»é¢ã‚’é…ç½®
#     if "messages" not in st.session_state:
#       st.session_state.messages = []
#     if user_input := st.chat_input('è³ªå•ã—ã‚ˆã†ï¼'):
#         # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
#         for message in st.session_state.messages:
#             with st.chat_message(message["role"]):
#                 st.markdown(message["content"])
#         print(user_input)
#         with st.chat_message('user'):
#             st.markdown(user_input)
#         st.session_state.messages.append({"role": "user", "content": user_input})
#         with st.chat_message('assistant'):
#             with st.spinner('Gemini is typing ...'):
#                 response = qa.invoke(user_input)
#             st.markdown(response['result'])
#             #å‚è€ƒå…ƒã‚’è¡¨ç¤º
#             doc_urls = []
#             doc_pdfs = []
#             for doc in response["source_documents"]:
#                 #æ—¢ã«å‡ºåŠ›ã—ãŸã®ã¯ã€å‡ºåŠ›ã—ãªã„
#                 if "source_url" in doc.metadata and doc.metadata["source_url"] not in doc_urls:
#                     doc_urls.append(doc.metadata["source_url"])
#                     st.markdown(f"å‚è€ƒå…ƒï¼š{doc.metadata['source_url']}")
#                 elif "source_pdf" in doc.metadata and doc.metadata["source_pdf"] not in doc_pdfs:
#                     doc_pdfs.append(doc.metadata["source_pdf"])
#                     st.markdown(f"å‚è€ƒå…ƒï¼š{doc.metadata['source_pdf']}")
#         st.session_state.messages.append({"role": "assistant", "content": response["result"]})


# if __name__ == "__main__":
#   main()